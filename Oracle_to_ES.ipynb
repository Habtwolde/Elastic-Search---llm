{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6946d79",
   "metadata": {},
   "source": [
    "### Imports & environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "080c6192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV loaded: True\n",
      "Oracle DSN: localhost:1521/XEPDB1\n",
      "Oracle User: es_user\n",
      "ES URL: http://localhost:9200\n",
      "ES INDEX: oracle_elser_index_v2\n",
      "ES PIPELINE: elser-oracle-pipeline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import oracledb\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "# Load .env from project root\n",
    "env_loaded = load_dotenv()\n",
    "print(\"ENV loaded:\", env_loaded)\n",
    "\n",
    "# ---- Oracle ----\n",
    "ORACLE_HOST = os.getenv(\"ORACLE_HOST\")\n",
    "ORACLE_PORT = os.getenv(\"ORACLE_PORT\")\n",
    "ORACLE_SERVICE = os.getenv(\"ORACLE_SERVICE\")\n",
    "ORACLE_USER = os.getenv(\"ORACLE_USER\")\n",
    "ORACLE_PASSWORD = os.getenv(\"ORACLE_PASSWORD\")\n",
    "\n",
    "# Build DSN\n",
    "ORACLE_DSN = f\"{ORACLE_HOST}:{ORACLE_PORT}/{ORACLE_SERVICE}\"\n",
    "\n",
    "print(\"Oracle DSN:\", ORACLE_DSN)\n",
    "print(\"Oracle User:\", ORACLE_USER)\n",
    "\n",
    "# ---- Elasticsearch ----\n",
    "ES_URL = os.getenv(\"ES_URL\", \"http://localhost:9200\")\n",
    "ES_USER = os.getenv(\"ES_USER\", \"elastic\")\n",
    "ES_PASS = os.getenv(\"ES_PASS\", os.getenv(\"ELASTIC_PASSWORD\"))\n",
    "\n",
    "ES_INDEX = os.getenv(\"ES_INDEX\", \"oracle_elser_index_v2\")\n",
    "ES_PIPELINE = \"elser-oracle-pipeline\"\n",
    "\n",
    "print(\"ES URL:\", ES_URL)\n",
    "print(\"ES INDEX:\", ES_INDEX)\n",
    "print(\"ES PIPELINE:\", ES_PIPELINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1892178",
   "metadata": {},
   "source": [
    "### Test Oracle connection + confirm row counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab1c1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle connected OK\n",
      "DOCS row count: 5\n",
      "\n",
      "Latest 5 rows (id, title, updated_at):\n",
      "('5', '5', datetime.datetime(2025, 2, 5, 0, 0))\n",
      "('4', '4', datetime.datetime(2025, 2, 1, 0, 0))\n",
      "('3', '3', datetime.datetime(2025, 1, 22, 0, 0))\n",
      "('2', '2', datetime.datetime(2025, 1, 15, 0, 0))\n",
      "('1', '1', datetime.datetime(2025, 1, 10, 0, 0))\n",
      "\n",
      "Oracle connection closed.\n"
     ]
    }
   ],
   "source": [
    "conn = oracledb.connect(user=ORACLE_USER, password=ORACLE_PASSWORD, dsn=ORACLE_DSN)\n",
    "print(\"Oracle connected OK\")\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT COUNT(*) FROM docs\")\n",
    "    total = cur.fetchone()[0]\n",
    "    print(\"DOCS row count:\", total)\n",
    "\n",
    "    # quick peek (latest 5)\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT id, title, updated_at\n",
    "        FROM docs\n",
    "        ORDER BY updated_at DESC NULLS LAST\n",
    "        FETCH FIRST 5 ROWS ONLY\n",
    "    \"\"\")\n",
    "    rows = cur.fetchall()\n",
    "    print(\"\\nLatest 5 rows (id, title, updated_at):\")\n",
    "    for r in rows:\n",
    "        print(r)\n",
    "\n",
    "conn.close()\n",
    "print(\"\\nOracle connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa15b66a",
   "metadata": {},
   "source": [
    "### Pull rows from Oracle and index them into Elasticsearch (with your ingest pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a7c3c5",
   "metadata": {},
   "source": [
    "#### First, confirm what pipelines exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57ead134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipelines found: 7\n",
      "- behavioral_analytics-events-final_pipeline\n",
      "- elser_oracle_pipeline\n",
      "- ent-search-generic-ingestion\n",
      "- logs-default-pipeline\n",
      "- logs@default-pipeline\n",
      "- logs@json-message\n",
      "- logs@json-pipeline\n"
     ]
    }
   ],
   "source": [
    "# STEP 4A: List ingest pipelines and confirm the correct pipeline id\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\n",
    "    ES_URL,\n",
    "    basic_auth=(ES_USER, ES_PASS),\n",
    "    request_timeout=120\n",
    ")\n",
    "\n",
    "pipelines = es.ingest.get_pipeline()\n",
    "print(\"Pipelines found:\", len(pipelines))\n",
    "for pid in sorted(pipelines.keys()):\n",
    "    print(\"-\", pid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83326c8c",
   "metadata": {},
   "source": [
    "#### Then, set the correct pipeline id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce3099e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ES_PIPELINE: elser_oracle_pipeline\n"
     ]
    }
   ],
   "source": [
    "# STEP 4B: Set correct pipeline id (most likely)\n",
    "\n",
    "ES_PIPELINE = \"elser_oracle_pipeline\"\n",
    "print(\"Using ES_PIPELINE:\", ES_PIPELINE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d4199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched docs from Oracle: 5\n",
      "Sample doc types: str str str str\n",
      "Sample doc preview: {'id': '5', 'title': '5', 'updated_at': '2025-02-05T00:00:00'}\n",
      "Sample body preview: Truck unloading accident resulting in minor material damage but no injuries....\n",
      "Bulk OK: 5\n",
      "Bulk Errors: 0\n",
      "ES count after ingest: 10\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Convert Oracle LOB/CLOB to string before sending to Elasticsearch\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "import oracledb\n",
    "from datetime import datetime\n",
    "\n",
    "ES_PIPELINE = \"elser_oracle_pipeline\"\n",
    "\n",
    "def lob_to_str(v):\n",
    "    \"\"\"Convert Oracle LOB/CLOB to Python str safely.\"\"\"\n",
    "    if v is None:\n",
    "        return \"\"\n",
    "    # oracledb LOB objects have .read()\n",
    "    if hasattr(v, \"read\"):\n",
    "        return v.read() or \"\"\n",
    "    return str(v)\n",
    "\n",
    "# --- Connect to ES\n",
    "es = Elasticsearch(\n",
    "    ES_URL,\n",
    "    basic_auth=(ES_USER, ES_PASS),\n",
    "    request_timeout=120\n",
    ")\n",
    "\n",
    "# --- Fetch docs from Oracle (id/title/body/updated_at)\n",
    "conn = oracledb.connect(user=ORACLE_USER, password=ORACLE_PASSWORD, dsn=ORACLE_DSN)\n",
    "\n",
    "docs = []\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT id, title, body, updated_at\n",
    "        FROM docs\n",
    "        ORDER BY updated_at DESC NULLS LAST\n",
    "        FETCH FIRST 500 ROWS ONLY\n",
    "    \"\"\")\n",
    "    for (doc_id, title, body, updated_at) in cur.fetchall():\n",
    "\n",
    "        # Normalize updated_at to ISO string\n",
    "        if isinstance(updated_at, datetime):\n",
    "            updated_iso = updated_at.isoformat()\n",
    "        else:\n",
    "            updated_iso = str(updated_at) if updated_at is not None else None\n",
    "\n",
    "        docs.append({\n",
    "            \"id\": str(doc_id),\n",
    "            \"title\": str(title) if title is not None else \"\",\n",
    "            \"body\": lob_to_str(body),               # IMPORTANT FIX\n",
    "            \"updated_at\": updated_iso\n",
    "        })\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"Fetched docs from Oracle:\", len(docs))\n",
    "if docs:\n",
    "    print(\"Sample doc types:\",\n",
    "          type(docs[0][\"id\"]).__name__,\n",
    "          type(docs[0][\"title\"]).__name__,\n",
    "          type(docs[0][\"body\"]).__name__,\n",
    "          type(docs[0][\"updated_at\"]).__name__)\n",
    "    print(\"Sample doc preview:\", {k: docs[0].get(k) for k in [\"id\",\"title\",\"updated_at\"]})\n",
    "    print(\"Sample body preview:\", (docs[0].get(\"body\",\"\")[:200] + \"...\") if docs[0].get(\"body\") else \"(empty)\")\n",
    "\n",
    "# --- Prepare bulk actions\n",
    "actions = [\n",
    "    {\n",
    "        \"_op_type\": \"index\",\n",
    "        \"_index\": ES_INDEX,\n",
    "        \"_id\": d[\"id\"],\n",
    "        \"pipeline\": ES_PIPELINE,\n",
    "        \"_source\": d\n",
    "    }\n",
    "    for d in docs\n",
    "]\n",
    "\n",
    "# --- Execute bulk\n",
    "ok, errors = bulk(es, actions, raise_on_error=False, raise_on_exception=False)\n",
    "print(\"Bulk OK:\", ok)\n",
    "print(\"Bulk Errors:\", len(errors))\n",
    "for e in errors[:5]:\n",
    "    print(e)\n",
    "\n",
    "# --- Refresh + count\n",
    "es.indices.refresh(index=ES_INDEX)\n",
    "print(\"ES count after ingest:\", es.count(index=ES_INDEX)[\"count\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921acdd",
   "metadata": {},
   "source": [
    "### STEP 6 â€” Verify the ingest pipeline actually ran (ELSER fields exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Verify ELSER enrichment exists on a known incident document\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "\n",
    "ES_PIPELINE = \"elser_oracle_pipeline\"\n",
    "ELSER_FIELD = \"ml.inference.body_expanded\"\n",
    "\n",
    "es = Elasticsearch(\n",
    "    ES_URL,\n",
    "    basic_auth=(ES_USER, ES_PASS),\n",
    "    request_timeout=120\n",
    ")\n",
    "\n",
    "doc_id = \"5\"  # pick one incident id you ingested\n",
    "\n",
    "resp = es.get(\n",
    "    index=ES_INDEX,\n",
    "    id=doc_id,\n",
    "    _source_includes=[\"id\", \"title\", \"body\", \"updated_at\", ELSER_FIELD, \"ml.inference_error\"]\n",
    ")\n",
    "\n",
    "src = resp.get(\"_source\", {})\n",
    "print(\"Found doc:\", src.get(\"id\"), \"| title:\", src.get(\"title\"))\n",
    "print(\"Has ELSER field?:\", ELSER_FIELD in src)\n",
    "\n",
    "# Show a small sample of the expanded tokens/weights if present\n",
    "expanded = src.get(\"ml\", {}).get(\"inference\", {}).get(\"body_expanded\", {})\n",
    "print(\"ELSER feature count:\", len(expanded))\n",
    "\n",
    "# Print top 20 features by weight (for sanity check)\n",
    "top20 = sorted(expanded.items(), key=lambda kv: kv[1], reverse=True)[:20]\n",
    "print(\"\\nTop 20 ELSER features:\")\n",
    "for k, v in top20:\n",
    "    print(f\"{k:20s} {v}\")\n",
    "\n",
    "# If any inference error was captured, show it\n",
    "inf_err = src.get(\"ml\", {}).get(\"inference_error\")\n",
    "if inf_err:\n",
    "    print(\"\\nINFERENCE ERROR:\", inf_err)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
